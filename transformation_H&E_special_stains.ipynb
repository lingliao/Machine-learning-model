{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNoJZTWWjP0hYYNcR1ZVRBM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lingliao/Machine-learning-model/blob/main/transformation_H%26E_special_stains.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ConfigObj"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BDEF6crrvo9t",
        "outputId": "cd1c54d8-bba0-40f0-8ae9-d96508f241fc"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ConfigObj\n",
            "  Downloading configobj-5.0.6.tar.gz (33 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from ConfigObj) (1.15.0)\n",
            "Building wheels for collected packages: ConfigObj\n",
            "  Building wheel for ConfigObj (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ConfigObj: filename=configobj-5.0.6-py3-none-any.whl size=34547 sha256=5ff5f8cca06a56f0d2b1cd58fc0172a37f1d39fa450d94ce727baaeeed99b0a8\n",
            "  Stored in directory: /root/.cache/pip/wheels/0d/c4/19/13d74440f2a571841db6b6e0a273694327498884dafb9cf978\n",
            "Successfully built ConfigObj\n",
            "Installing collected packages: ConfigObj\n",
            "Successfully installed ConfigObj-5.0.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from configobj import ConfigObj\n",
        "from time import time, sleep\n",
        "import data_loader\n",
        "import network\n",
        "import tensorflow as tf\n",
        "from tqdm import tqdm\n",
        "import glob, ops, sys\n",
        "import numpy as np\n",
        "import random\n",
        "from network import _YCbCr2RGB, _normalize, sobelFilter"
      ],
      "metadata": {
        "id": "h-aB-Wbq6130"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rgb2ycbcr(image):\n",
        "    image_temp = image\n",
        "    X = 0.299*image_temp[:,:,:,0]+0.587*image_temp[:,:,:,1]+0.114*image_temp[:,:,:,2]\n",
        "    Y = (image_temp[:,:,:,0] - X)*0.713 + 128\n",
        "    Z = (image_temp[:,:,:,2] - X)*0.564 + 128\n",
        "\n",
        "    X = tf.expand_dims(X, axis=3)\n",
        "    Y = tf.expand_dims(Y, axis=3)\n",
        "    Z = tf.expand_dims(Z, axis=3)\n",
        "    return tf.concat([X, Y, Z], axis=3)\n",
        "\n",
        "\n",
        "def ycbcr2rgb(image):\n",
        "    X = image[...,0] + 1.403* (image[...,1] - 128)\n",
        "    Y = image[...,0] - 0.714* (image[...,1] - 128) - 0.344*(image[...,2] - 128)\n",
        "    Z = image[...,0] + 1.773* (image[...,2] - 128)\n",
        "    X = tf.expand_dims(X, axis=3)\n",
        "    Y = tf.expand_dims(Y, axis=3)\n",
        "    Z = tf.expand_dims(Z, axis=3)\n",
        "    return tf.concat([X, Y, Z], axis=3)\n",
        "\n",
        "def init_parameters():\n",
        "    tc, vc = ConfigObj(), ConfigObj()\n",
        "    tc.is_training, vc.is_training = True, False\n",
        "    tc.batch_size, vc.batch_size = 12, 12\n",
        "    tc.n_channels, vc.n_channels = 16, 16\n",
        "    tc.image_size, vc.image_size = 256, 256\n",
        "    tc.n_threads, vc.n_threads = 2, 1\n",
        "    tc.n_blocks, vc.n_blocks = 5, 5\n",
        "    tc.n_levels, vc.n_levels = 4, 4\n",
        "    tc.checkpoint = 1000\n",
        "    tc.q_limit = 1000\n",
        "    tc.lamda = 2000.0\n",
        "    return tc, vc"
      ],
      "metadata": {
        "id": "oCMN_MWu617-"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "if __name__ == '__main__':\n",
        "    #Choose the location of the training and validation images\n",
        "    train_images = glob.glob('image_dataset/training/label/*.tif')\n",
        "    valid_images = glob.glob('image_dataset/validation/label/*.tif')\n",
        "\n",
        "    print(train_images)\n",
        "    random.shuffle(train_images)\n",
        "\n",
        "    train_config, valid_config = init_parameters()\n",
        "    patch_size = train_config.image_size\n",
        "    valid_config.q_limit = 500\n",
        "\n",
        "    with tf.Graph().as_default(), tf.device('/cpu:0'):\n",
        "\n",
        "        input_ = tf.placeholder(tf.float32, shape=[None, patch_size, patch_size, 3])\n",
        "        label_ = tf.placeholder(tf.float32, shape=[None, patch_size, patch_size, 3])\n",
        "        train_bl = data_loader.TrainBatchLoader(train_images, input_, label_, train_config)\n",
        "        valid_bl = data_loader.ValidBatchLoader(valid_images, input_, label_, valid_config)\n",
        "\n",
        "        train_x, train_y = train_bl.get_batch()\n",
        "        valid_x, valid_y = valid_bl.get_batch()\n",
        "\n",
        "        device = ops.get_available_gpus()[0]\n",
        "        with tf.device(device):\n",
        "\n",
        "            with tf.variable_scope('Generator'):\n",
        "                G = network.Generator(train_x, train_config)\n",
        "\n",
        "            with tf.variable_scope('Discriminator'):\n",
        "                D_fake = network.Discriminator(G.output, train_config)\n",
        "\n",
        "            with tf.variable_scope('Discriminator', reuse=True):\n",
        "                D_real = network.Discriminator(train_y, train_config)\n",
        "            with tf.variable_scope('Generator', reuse=True):\n",
        "                valid_G = network.Generator(valid_x, valid_config)\n",
        "\n",
        "            with tf.variable_scope('Discriminator', reuse=True):\n",
        "                valid_D_fake = network.Discriminator(valid_G.output, valid_config)\n",
        "                valid_D_real = network.Discriminator(valid_y, valid_config)\n",
        "                valid_D_fake_loss = tf.reduce_mean(tf.square(valid_D_fake.output))\n",
        "                valid_D_real_loss = tf.reduce_mean(tf.square(1 - valid_D_real.output))\n",
        "\n",
        "            valid_G_mse_loss = tf.reduce_mean(tf.abs(valid_y - valid_G.output))\n",
        "            valid_G_tv_loss  = tf.reduce_mean(tf.image.total_variation(valid_G.output)) / (3*(patch_size ** 2))\n",
        "            valid_G_dis_loss = tf.reduce_mean(tf.square(1 - valid_D_fake.output))\n",
        "            valid_G_loss = valid_G_mse_loss + 0.02 * valid_G_tv_loss + train_config.lamda * valid_G_dis_loss\n",
        "\n",
        "            D_fake_loss = tf.reduce_mean(tf.square(D_fake.output))\n",
        "            D_real_loss = tf.reduce_mean(tf.square(1 - D_real.output))\n",
        "            D_loss = D_fake_loss + D_real_loss\n",
        "\n",
        "            G_mse_loss = tf.reduce_mean(tf.abs(train_y - G.output))\n",
        "            G_tv_loss  = tf.reduce_mean(tf.image.total_variation(G.output)) / (3*(patch_size ** 2))\n",
        "            G_dis_loss = tf.reduce_mean(tf.square(1 - D_fake.output))\n",
        "            G_loss = G_mse_loss + 0.02 * G_tv_loss + train_config.lamda * G_dis_loss\n",
        "\n",
        "            gen_var_list = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='Generator')\n",
        "            dis_var_list = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='Discriminator')\n",
        "\n",
        "            G_train_step = tf.train.AdamOptimizer(1e-4).minimize(G_loss, var_list=gen_var_list)\n",
        "            D_train_step = tf.train.AdamOptimizer(1e-5).minimize(D_loss, var_list=dis_var_list)\n",
        "\n",
        "            tf.summary.image('train_HE', ycbcr2rgb(train_x))\n",
        "            tf.summary.image('train_PAS', ycbcr2rgb(train_y))\n",
        "            tf.summary.image('train_G_output', ycbcr2rgb(G.output))\n",
        "\n",
        "            tf.summary.image('valid_HE_prev', ycbcr2rgb(valid_x))\n",
        "            tf.summary.image('valid_HE_new', ycbcr2rgb(valid_y))\n",
        "\n",
        "            tf.summary.image('valid_HE_prev_output', ycbcr2rgb(valid_G.output))\n",
        "               \n",
        "            tf.summary.scalar('D_fake_loss', D_fake_loss)\n",
        "            tf.summary.scalar('D_real_loss', D_real_loss)\n",
        "            tf.summary.scalar('D_loss', D_loss)\n",
        "            tf.summary.scalar('G_dis_loss', G_dis_loss)\n",
        "            tf.summary.scalar('G_mse_loss', G_mse_loss)\n",
        "            tf.summary.scalar('G_tv_loss', G_tv_loss)\n",
        "            tf.summary.scalar('G_loss', G_loss)\n",
        "\n",
        "        with tf.Session(config=tf.ConfigProto(allow_soft_placement=True)) as sess:\n",
        "            sess.run(tf.global_variables_initializer())\n",
        "            saver = tf.train.Saver(max_to_keep=0)\n",
        "\n",
        "            merged = tf.summary.merge_all()\n",
        "\n",
        "            model_name = 'Models'\n",
        "            summary_step = 1000\n",
        "            if os.path.exists(\"tensorboard/\" + model_name):\n",
        "                shutil.rmtree(\"tensorboard/\" + model_name)\n",
        "            train_writer = tf.summary.FileWriter(\"tensorboard/\" + model_name, sess.graph)\n",
        "\n",
        "\n",
        "            tf.train.start_queue_runners(sess=sess)\n",
        "            train_bl.start_threads(sess, n_threads=train_config.n_threads)\n",
        "            valid_bl.start_threads(sess, n_threads=valid_config.n_threads)\n",
        "            for i in tqdm(range(30)): sleep(1)\n",
        "            print(train_bl.queue.size().eval(), valid_bl.queue.size().eval())\n",
        "\n",
        "            #Save loss information in a validation log.\n",
        "            valid_log = open('valid_log.txt', 'w')\n",
        "\n",
        "            n_eval_steps = valid_config.q_limit // valid_config.batch_size\n",
        "            check = train_config.checkpoint\n",
        "            min_loss = float('inf')\n",
        "            start_time = time()\n",
        "            for x in range(1,100):\n",
        "                d_fake_loss, d_real_loss, g_loss = 0, 0, 0\n",
        "                NumGen = max(3, int(7-x/4))\n",
        "                for i in range(check):\n",
        "                    for j in range(NumGen):\n",
        "                        _, b = sess.run([G_train_step, G_loss])\n",
        "                        g_loss += b\n",
        "                    _, a1, a2 = sess.run([D_train_step, D_fake_loss, D_real_loss])\n",
        "                    d_fake_loss += a1\n",
        "                    d_real_loss += a2\n",
        "                    if not i % summary_step:\n",
        "                        summary_train = sess.run(merged)\n",
        "                        train_writer.add_summary(summary_train, (x * check + i) * (1))\n",
        "                res = np.mean([sess.run([valid_G_loss, valid_G_mse_loss, valid_G_tv_loss, valid_G_dis_loss, valid_D_fake_loss, valid_D_real_loss]) for _ in range(n_eval_steps)], axis=0)\n",
        "\n",
        "                format_str = ('iter: %d valid_G_loss: %.3f valid_G_mse_loss: %.3f valid_G_tv_loss: %.3f valid_G_dis_loss: %.3f valid_D_fake_loss: %.3f valid_D_real_loss: %.3f train_G_loss: %.3f train_D_fake_loss: %.3f train_D_real_loss: %.3f time: %d')\n",
        "                text = (format_str % (x*check, res[0], res[1], res[2], res[3], res[4], res[5], g_loss/(check*NumGen), d_fake_loss/check, d_real_loss/check, int(time()-start_time)))\n",
        "                ops.print_out(valid_log, text)\n",
        "                #Save the model.\n",
        "                saver.save(sess, 'Models/{}'.format(x*check))\n",
        "                if res[1] < min_loss:\n",
        "                    min_loss = res[1]\n",
        "                    #Save the model with the lowest validation L1 loss.\n",
        "                    saver.save(sess, 'Models/best_model')"
      ],
      "metadata": {
        "id": "VWIA2PPu62AG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qTvrOe0U62D1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}